---
layout: default
---


许久没写过笔记，最近在跟进Google刚刚开源的自家使用Tensorflow的ML学习教程，将ML从头捋一捋。


----------
### 训练和损失 ###

训练模型表示通过有标签样本来学习（确定）所有权重和偏差的理想值。在监督式学习中，机器学习算法通过以下方式构建模型：检查多个样本并尝试找出可最大限度地减少损失的模型；这一过程称为经验风险最小化。

损失是对糟糕预测的惩罚。也就是说，损失是一个数值，表示对于单个样本而言模型预测的准确程度。如果模型的预测完全准确，则损失为零，否则损失会较大。训练模型的目标是从所有样本中找到一组平均损失“较小”的权重和偏差。例如，图 3 左侧显示的是损失较大的模型，右侧显示的是损失较小的模型。关于此图，请注意以下几点：



- 红色箭头表示损失。


- 蓝线表示预测。

![](https://i.imgur.com/lfuVQpE.png)

接下来是介绍几种常见的损失函数

1.平方损失

单个样本的平方损失如下：

	= the square of the difference between the label and the prediction
  	= (observation - prediction(x))2
  	= (y - y')2

2.均方误差（MSE）

指的是每个样本的平均平方损失。要计算 MSE，请求出各个样本的所有平方损失之和，然后除以样本数量：

![](https://i.imgur.com/5cge4K9.png)

![](https://i.imgur.com/wsPastB.png)

## 降低LOSS ##
现在我们可以得到训练好的模型，那么我们如何可以降低模型的训练损失，从而得到一个更好的模型？

### 梯度下降 ###

梯度下降法是我们现在使用最多的机器学习降低模型损失的方法，相较于迭代方法（仅是胡乱猜测权值），梯度下降则更有目的性，更快可以获得最优的模型。

假设我们有时间和计算资源来计算 w1 的所有可能值的损失。对于我们一直在研究的回归问题，所产生的损失与 w1 的图形始终是凸形。换言之，图形始终是碗状图，如下所示：

![](https://i.imgur.com/v54oS4u.png)

凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。

通过计算整个数据集中 w1 每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为梯度下降法。

梯度下降法的第一个阶段是为 w1 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 w1 设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：

![](https://i.imgur.com/PYmXsUS.png)

然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 3 所示）就等于导数。

请注意，梯度是一个矢量，因此具有以下两个特征：

- 方向

- 大小

梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。

![](https://i.imgur.com/B0wSUKw.png)

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：

![](https://i.imgur.com/TTrPW5C.png)

然后，梯度下降法会重复此过程，逐渐接近最低点。

这个就是梯度下降获得最优模型的原理。

#### 偏导数和梯度 ####

关于梯度下降，其中最重要的是偏导数和梯度的概念。从数学上解释：

![](https://i.imgur.com/MkIDlzF.png)

![](https://i.imgur.com/q6XlJq9.png)

![](https://i.imgur.com/tBHlRYu.png)

![](https://i.imgur.com/CwFJ5zf.png)
